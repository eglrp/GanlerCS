# 信息论 - 第二章复习

[TOC]

## 信息熵

$$
H(x)=-\mathbb E\left[\log(P(x))\right]=-\sum_{i=1}^np(x_i)\log(p(x_i))\\
$$

- 非负性；
- 对称性；
- 可加性；

## 条件熵

$$
H(Y|X)=\sum_{i,j=1}^np(x_i,y_j)\log \left(\frac{p(x_i,y_j)}{p(x_i)}\right)=H(X,Y)-H(X)=H(Y)-H(Y|X)
$$

$$
H(X)\ge H(X|Y)
$$

## 联合熵

$$
H(X,Y)=H(X)+H(Y|X)=-\sum_{x\in X}\sum_{y\in Y}P(x,y)\log P(x,y)
$$

## 最大离散熵定理

当X是等概分布的时候熵最大为$\log |X|$（离散情况下均匀分配是上限，连续情况下的正态分布微分熵最大）。

## 互信息

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/256px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png)
$$
I(X;Y)=I(Y;X)=\sum_{x\in X} \sum_{y\in Y} p(x,y) \left(\log \frac{p(x,y)}{p(x)p(y)}\right)\ge 0
$$

> 记不清分子分母那就考虑"非负性"。
>
> 注意$\frac{p(x,y)}{p(x)}=p(y|x)$

重要变形（根据上图）：
$$
I(X;Y)=H(X)-H(Y|X)=H(X)+H(Y)-H(X,Y)\ge 0
$$

> 被减去的部分也被称作**信道疑义度/可疑度（损失熵）**。<u>代表在信道传输过程中的损失量。</u>

### 例题

![](https://i.loli.net/2019/06/04/5cf6293154d4b26805.png)

> 信道两端成2个离散随机变量：
> $$
> I(X;Y)=\sum_{i=0}^1\sum_{j=0}^1 p(x_i,y_j)\log (\frac{ p(x_i,y_j)}{p(x_i)p(y_j)})
> $$
>
> |             | P(x=0)=3/4 | P(x=1)=1/4  |
> | ----------- | ---------- | ----------- |
> | P(Y=0)=5/12 | P(0,0)=3/8 | P(1,0)=1/24 |
> | P(Y=1)=7/12 | P(0,1)=3/8 | P(1,1)=5/24 |
>
> $$
> \frac{3}{8}\log\frac{3/8}{3/4*5/12}+\frac{1}{24}\log\frac{1/24}{1/4*5/12}+\frac{3}{8}\log\frac{3/8}{3/4*7/12}+\frac{5}{24}\log\frac{5/24}{1/4*7/12}
> $$

## Jenson不等式

对于下凸函数f：
$$
f(\frac{\sum x_i}{n})\ge \frac{\sum f(x_i)}{n}
$$

> $$
> f(\mathbb E(X))\ge \mathbb E(f(X))
> $$

## 扩展信源

原来的信源是X（$x_1,\cdots,x_m$），现在是$X^N$。那么信息熵为$NH(X)$。

扩展多少次，系统信息熵翻多少倍。

**平均符号熵**：信息熵/符号数；

## 极限熵

$$
H_\infin=\lim_{N\to\infin}\frac{H(X_1,X_2,\cdots,X_N)}{N}
$$

## 马尔可夫信源

- 可选状态：$S=\{e_1,\cdots,e_{n^m}\}$；
- 可选符号：$X=\{x_1,\cdots,x_n\}$；
- 输出的随机符号序列：$\{X_1,\cdots,X_n\}$
- 所处状态序列：$\{S_1,\cdots,S_n\}$

- t时间，状态$e_i$，输出$x_k$的概率：$p_t(x_k|e_i)=p(X=x_k|S=e_i)$
- t-1时处于$e_i$，t时间处于$e_j$的概率：$p(e_j|e_i)=p(S=e_j|S=e_i)$
- 一阶马尔可夫信源：$H_2=H_{1+1}=H(X_2|X_1)$

![](https://i.loli.net/2019/06/13/5d025975efdc818081.png)

$p(x_{k_1}x_{k_2}\cdots x_{k_{m+1}})=p(e_j)=p(e_i)p(e_j|e_i)$

### 成为马尔可夫信源的条件

1. 某时刻的<u>符号输出</u>只与<u>此时的信源状态</u>有关，与之前的信源和符号输出无关。

> 若具有**时齐**性：
> $$
> p_t(x_k|e_i)=p(x_k|e_i)
> $$
> 不论何时，固定状态下的符号出现概率不变。

2. t时刻的状态，取决于t-1时刻的状态与t时刻的输出符号。

$$
p(e_j|e_i)=p(x_k|e_i)
$$

### 平稳

即符号的概率不随时间变化而变化。(符号概率与时间无关)

> 对于非平稳信源熵，假设其平稳，求极限熵。

### 表示方法

![](https://i.loli.net/2019/06/04/5cf65ed2149b420684.png)

> - 状态2符号；
>
> - 状态2状态；

### 信源熵

m阶**马尔可夫信源稳定**后的状态极限概率：
$$
\begin{aligned}
H_\infin=H_{m+1}=-\sum\sum p(e_i)\log(e_i|e_j)=-\sum\sum p(e_j)p(e_i|e_j)\log p(e_i|e_j)=-\sum\sum p(e_j)H(e_j|e_i)\\
\end{aligned}
$$

> 所以计算上图的马尔可夫信源稳定后的信源熵是：
>
> 首先$p(a|b)$容易算，直接就是状态b到a的一个概率（对应状态转移矩阵）；
>
> 重点是如何算**状态极限概率：**$p(a)$：
> $$
> p(e_j)=\sum p(e_i)p(e_j|e_i)
> $$
> 按照这个公式，加上$\sum p(e_j)=1$，**解方程**就可算得结果。

> 换一个角度去思考就是：
>
> 取一个状态，固定这个状态，算这个系统下的信息熵。对于所有状态求和。

### 冗余度

![](https://i.loli.net/2019/06/13/5d01b59cf323316495.png)

### 相对熵

$$
\eta=\frac{H(X)}{H_\max(X)}=\frac{H_\infin}{H_0}
$$

> 分子：信源实际熵（可以直接认为是信息熵）；
>
> 分母：符号等概率且无相关性的理想离散信源熵；(定值)

### 冗余度

$$
E=1-\eta
$$

### 信息变差

> 这不就是p和u的KL散度吗(说人话不好吗。。。)

$$
I_{0\infin}=H_\max(X)-H(X)=H_0-H_\infin
$$

## 习题总结

#### 自信息和信息熵

- 事件 $\to$ 自信息；
- 某一现象是否发生 $\to$ 信息熵；

#### "独立变化"：独立部分直接相加

若一个像素有256个不同电平，那么一个像素的信息量：8bit

对于一个M×N的像素块，如果像素之间**独立变化**，那么气信息量可以直接加起来：8×M×N

#### $X^N$表示N个X符号

当前后发出的信息互不干扰的情况下，信源熵$H(X^N)=NH(X)$。

#### 对某个信息重复传输会降低信息熵，提升冗余度

注意信息熵的单位是`bit/sign`。重复N遍传输某个信息会让该信息的信息熵降低N倍。

> 重复偶数倍数往往不太合理，因为当错误的码元数量呈1:1的时候，无法判定正确与否。(主要指二元的情况)


#### 当前后符号呈相关性的时候，信息熵减小，冗余度增大。