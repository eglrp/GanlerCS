[TOC]

### 条件熵

#### 复习

> 之前内容的快速复习：
> $$
> I(x)=-\log P(x)\\
> H(x)={\rm E\left(I(x)\right)=E}(-\log P(x))\\
> H(x,y)={\rm E(I(x,y))}=\sum_{x\in X}\sum_{y\in Y}-P(x,y)\log P(x,y)
> $$
> 条件概率快速复习：
>
> $P(Y|X)=\frac{P(Y)}{P(X)}$，即在X发生的情况下，Y发生的概率。

#### 继续

$$
(X,Y)\sim P(x,y)\\
H(Y|X)=\sum_{x\in X}P(x)H(Y|X=x)=-\sum_{x\in X,y\in Y}P(x,y)\log\frac{P(x,y)}{P(x)}\\
or\\
H(Y|X)=\sum_{x\in X}\left(p(x)\left(\sum_{y\in Y}q(y|x)\log(-q(y|x))\right)\right)
$$

条件熵$H(Y|X)$可解释为：

取遍所有X的取值，在这些取值下计算x与y的信息熵的期望。

#### 条件熵的性质

- 当X与Y独立分布时，$H(X|Y)=H(X),H(Y|X)=H(Y)$。

> **解释**：当X与Y独立时，两者互不干扰，换句话说就是，此时，观察X得不到Y的统计信息，知道X($H(Y|X)$ )和不知道X($H(Y)$)的效果一样。

#### 证明

$$
H(X,Y)=H(X)+H(Y|X)
$$

> **理解**：X与Y提供的信息量=X提供的信息量+已知X，Y提供的信息量。

$$
H(x,y)= -\mathbb E[\log(P(x,y))]\\
H(x)=-\mathbb E\left[\log(P(x))\right]\\
H(Y|X)=-\mathbb E[\log\frac{P(x,y)}{P(x)}]
$$

写出这些式子，带入证明式中，即证毕。

#### 推论

$$
H(X,Y|Z)=H(X,Z)+H(Y|X,Z)\tag{4}
$$

$$
\begin{aligned}
H(X_1,X_2,\cdots,X_n)&=H(X_1)+H(X_n,\cdots,X_2|X_1)\\
&=H(X_1)+H(X_2|X_1)+H(X_n,\cdots,X_3|X_2X_1)\\
&=\sum_{i=1}^n H(X_i|X_i-1,\cdots,X_1)
\end{aligned}\tag{5}
$$

> 换个角度理解上述公式，我们要向得到$X_1\sim X_n$的信息，不妨将其视作一个信息序列，每次我们只接受一个信息$X_i$，但在此之前我已经知道了$X_{i-1}\sim X_1$了，将这些信息进行求和我们就得到了最后的信息量。

结合$(3)$与$(5)$，我们知道，当$X_1,\sim,X_n$相互独立的时候，$H(X_1,\cdots,X_n)=\sum H(X_i)$。

### 信息熵的性质

#### 对称性

$$
H(X_1,X_2,\cdots,X_n)=H(X_{k_1},X_{k_2},\cdots,X_{k_n})
$$

$\forall\ \ i,j\in[1,n],i\neq j$，有$k_i\neq k_j$。

即$X_i$ 的顺序不影响其信息熵的结果。

#### 非负性

$$
H(*)\ge 0
$$

这个上有没有信息量的话(废话/假话等)，但没有能让你失去信息的话。

#### 可加性

$$
H(X,Y)=H(X)+H(Y|X)
$$

#### 条件减少熵

$$
H(X|Y)\le H(X)
$$

> 同样的信息，不带前提条件（先验知识）的情况下（消除不确定性所需要的）信息量最大。
>
> 换个思维来理解，先验知识不会增加其他问题对应的信息熵。
>
> 其中当X和Y独立的时候取等号 。
>
> **注意**，统计平均意义上的条件确实能减少不确定性，但针对具体的条件信息(具体的Y的取值)则不一定意味着有：$H(X|Y=y)\le H(X)$。

**证明**：

> 首先介绍一下**互信息**的概念：
>
> > [ref-wiki](https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF)
>
> 在[概率论](https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E8%AE%BA)和[信息论](https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA)中，两个[随机变量](https://zh.wikipedia.org/wiki/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F)的**互信息**（Mutual Information，简称MI）或**转移信息**（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息是[点间互信息](https://zh.wikipedia.org/w/index.php?title=%E7%82%B9%E9%97%B4%E4%BA%92%E4%BF%A1%E6%81%AF&action=edit&redlink=1)（PMI）的期望值。

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/256px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png)
$$
I(X;Y)=I(Y;X)=\mathbb E\left(\log\frac{P(x,y)}{P(x)P(y)}\right)
$$


> 其实由上图，我们就可以知道$I(X,Y)\ge 0$了。

开始证明：

即证互信息大于0 ：
$$
\begin{aligned}
I(X;Y)&=H(X)-H(X|Y)\\
&=\mathbb E(-\log P(x))-\mathbb E\left(-\log\frac{P(x,y)}{P(y)}\right)\ge0
\end{aligned}
$$
亦即证：
$$
\mathbb E\left(\log P(x)\right)+\mathbb E\left(\log P(x)\right)\le\mathbb E(\log P(x,y))
$$
由琴生不等式：
$$
f(\frac{\sum x_i}{n})\ge \frac{\sum f(x_i)}{n}
$$
即
$$
f(\mathbb E(x))\ge\mathbb E(f(x))
$$
所以即证
$$
H(x,y)=\mathbb E(\log P(x,y))\ge0
$$
由信息熵的非负性，上述公式成立。



### 互信息

正如同上面所言：

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/256px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png)
$$
I(X;Y)=I(Y;X)=\mathbb E\left(\log\frac{P(x,y)}{P(x)P(y)}\right)
$$

> **解释**：<u>互信息刻画的是随机变量X与Y之间的相关程度。</u>

#### 从信道的角度理解互信息

信息转移：
$$
X->信道->Y
$$
信道可以看做是某种噪声下的概率转移矩阵$Q$，代表信息传输的过程。**没有噪声的情况下，转移矩阵是单位矩阵，而有噪声的情况下，转移矩阵里面是一些概率值。**
$$
Q=\{I(x;y)\}
$$
从信道的角度，<u>互信息$I(X;Y)$代表的就是该信道的**传输能力**。</u>

#### 可加性定理

$$
I(X_1,X_2,\cdots,X_n;Y)=\sum_{i=1}^n I(X_i;Y|X_1,X_2,\cdots,X_{i-1})
$$

#### <u>习题：硬币问题</u>

25枚硬币中有1枚假币，假币的质量比真币要轻，现有一块天平，可以乘凉两份硬币的大小偏向，问至少多少次称量可以确定假币。

解答：

令X为找出假币所需要的信息量，$Y^n$代表进行n次称量实验。
$$
\begin{aligned}
&H(X)=\log25\\
&I(Y^n;X)\le H(Y^n)\le\sum_{i=0}^n H(Y_i)\le n\log 3\\
&\log 25 \le n\log 3
\end{aligned}
$$

因为n是整数，所以n至少为3。

### 交叉熵(KL散度)

<u>用于描述两个分布之间的距离。</u>(两个分布的关联性，交叉熵越高，关联性越搞)
$$
D(\mathbf{p}||\mathbf{q})=\sum_{x\in X}p(x)\log\frac{p(x)}{q(x)}
$$

#### 特性

- **非负**，当两者分布相同时，值为0；
- **不满足对称性和三角不等式**；(p和q互换结果不一样)

#### 与信息熵之间的关系

$$
H(X)=\log N-D(\mathbf{p}||\mathbf{u})
$$

u即为X的等概率分布(假设$P(X_i)=\frac{1}{N}$)。

> 假设X满足均匀分布，那么其熵为$H'(x)=\log N$，由此可见，信息熵的数值其实就是当所有事件呈等概分布的时候，等概分布对应的熵减去原分布和等概分布的KL散度。这也是为什么说，当事件呈等概分布的时候，信息熵最大。

#### 与互信息之间的关系

> 我们说，互信息描述的是两个系统中，信息传输的能力。那么交叉熵(KL散度/距离)可以用来描述这一点。所以互信息可以被表示为：

$$
I(x;y)=D(p(x,y)||p(x)p(y))
$$

> 一般来说，我们不知道两个系统/分布的随机变量之间的关系，此时我们会假设其相互独立，这个时候计算概率我们就可以用$p(x,y)=p(x)p(y)$。而很多时候其并不独立，此时我们用互信息来描述当前关联性和相互独立之间的"距离"。距离为0的时候，说明完全独立，那么互信息也是0。