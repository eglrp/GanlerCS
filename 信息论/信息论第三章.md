# 信息论第三章 

### 信道

![img](https://i.loli.net/2019/06/14/5d0399c77c35d46350.png)



> $p(X|Y)$是信道转移概率。

​	

#### 对于事件的互信息

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/256px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png)
$$
I(x_i;y_i)={\rm I}(x_i)-{\rm I}(x_i|y_i)=-\log p(x_i)+\log p(x_i|y_i)
$$

> 注意被减者是${\rm I}(x_i|y_i)$。(通过这个图记忆即可)
>
> 注意这个不是指离散变量的那个互信息。

> 物理意义：
>
> > - 信宿端接收到消息$y_i$前后的信息量之差。(不确定程度的减少量)
> > - <u>信宿端接收到消息$y_j$后获得的关于信源消息$x_i$的**信息量**</u> （常考，看到“接收到yy后”获得关于信源xx的信息量就是指互信息）
>
> 对应到离散变量即：
> $$
> I(X;Y)=\sum_{x\in X }\sum_{y \in Y}p(x,y)\log {\rm I(x;y)}=\sum_{x\in X }\sum_{y \in Y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\ge 0
> $$
>

#### 例题

![](https://i.loli.net/2019/06/14/5d03a1925124321842.png)



> 解答过程：
> $$
> p(y_j)=\sum_{i=1}^2p(x_i,y_j)\\
> p(y_1)=\frac{7}{12}\\
> p(y_2)=\frac{5}{12}
> $$
> 
> $$
> I(X;Y)=\sum_{i=1}^2\sum_{j=1}^2p(x_i,y_j)\log \frac{p(y_j|x_i)}{p(y_j)}\\
> =\frac{5}{24}\log \frac{5/6}{7/12}+\frac{1}{24}\log \frac{1/6}{5/12}+\frac{3}{8}\log \frac{1/2}{7/12}+\frac{3}{8}\log \frac{1/2}{5/12}
> $$
> 

#### 损失熵

$$
I(X;Y)=H(X)-H(X|Y)
$$

$H(X|Y)$也被称作损失熵。(信道疑义度、可疑度)。

#### 信宿熵

> 接受端的信息量；

$$
H(Y)
$$

#### 噪声熵

> 发出消息X后，对信宿Y存在的平均不确定度。

$$
H(Y|X)
$$

![](https://i.loli.net/2019/06/14/5d03a79789b5078293.png)

#### 差错率

> 1-正确率；
>
> 正确率就是发啥收啥。发送1接受1；发送2接受2；

#### 一个特例

若一个通信系统满足：

- 二元信源
- 等概率分布
- 信道对称

则误差率即为$p(1/0)=p(0/1)$。

（二元对称情况即可，不需要一定等概分布，其中p为概率转移矩阵中的任意一项的值）其噪声熵为：$H(X|Y)=-\{p\log p+(1-p)\log (1-p)\}$

### 信道容量

#### 信源熵速率

单位时间内输出的熵；(bit/sec)
$$
nH(X)
$$

#### 接受熵速率

单位时间内接受的熵；
$$
R=nI(X;Y)
$$

#### 离散熵速率

每秒输出的符号数×信息熵

#### 连续熵速率

$$
H(X)=-\int p(x)\log p(x)dx
$$

若信源输出带宽有限，最大为W，那么采样率至少为2W，则其熵速率：
$$
H'(X)=-2WH(X)
$$


#### 信道容量 C

信道对于一切可能的概率分布而言能够传送的**最大熵速率**；

- 无损无噪：one 2 one
- 无损有噪：one 2 many(如图)

![](https://i.loli.net/2019/06/15/5d04653436b2b42641.png)

#### 离散无损信道的信道容量

> 用来传递离散消息的信道，称为离散信道；

- 离散信源符号数：N
- 符号间无相关性且等概分布；

则输出熵最大，为$H_\max(X)=\log N$

若离散信道最多每秒传送n个信源符号，则最大熵速率，即信道容量为$nH_\max(X)$

> 信道容量是可能传送的**最大熵速率**，实际情况下传信率可能远远低于信道容量。
>
> 原因:
>
> - 信源符号之间有相关性；
> - 符号非等概分布；
>
> > 解决问题的方法：最佳编码。



#### 离散有噪有损信道的信息速率

- 信源熵速率 > 信宿接收熵速率（有损）；
- 实际传信率 = 信宿接收熵速率；
- 信道容量 = 最大接收熵速率；

![](https://i.loli.net/2019/06/15/5d046849bf55929605.png)

![](https://i.loli.net/2019/06/15/5d046857cb88b71771.png)

#### 有损信道的接收熵速率

> 这个公式是万能的。

$$
R=nI(X;Y)=I'(X;Y)
$$

#### 连续有噪有损信道的信道容量

- 输入信号平均功率受限为P 
- 信道带宽为宽度为W的矩形 
- 信道中干扰噪声平均功率为N

y是接收端，x是发送端，n是与x不相关的噪音。
$$
y(t)=x(t)+n(t)
$$

$$
C=R_\max=\max\{I(X;Y)\}=\max\{H'(Y)-H'(Y|X)\}=\max\{H'(Y)-H'(N|X)\}=\max\{H'(Y)-H'(N)\}
$$

当输入信号幅度为**高斯分布**时（同时$H(Y)=H(X)$），有$\max H'(Y)=W\log 2\pi e (P+N)$。这样可推得C
$$
C=W \log \left(1+\frac{P}{N}\right)
$$

> 平均功率受限的高斯白噪声连续信道，其信道容量与信道带宽W、功率 **信噪比$\frac{P}{N}$** 有关。其值越大，信道容量越大。
>
> <u>连续时候的高斯分布</u>即<u>离散时候的均匀分布</u>。高斯白噪声的危害最大，因为其具有最大噪声熵速率。

### 复习不动，直接记结论：

对于一个WTP的信源，其对应的维度是2WT，其对应的半径(若平均功率相同)是$\sqrt {2WTP}$。

![](https://i.loli.net/2019/06/15/5d047f85f17e030895.png)

#### Shannon编码定理

存在一种足够复杂的编码方法，使系统能在差错率任意小的情况下实现理论上的最大传信率，而这样的系统就是理想通信系统。

### 扩频以改善信噪比

> $W_c$是扩频后的带宽，$W_s$是原始带宽。
>
> 信噪比，the higher, the better.

#### 输入端

$$
C_i=W_c\log \left\{1+\left(\frac{P}{N}\right)_i\right\}
$$

#### 输出端

$$
C_o=W_s\log\left\{1+\left(\frac{P}{N}\right)_o\right\}
$$

> 若传递过程中，不会丢失任何信息，则$C_i=C_o$。

> 改善技术是有限的，可见采用扩频技术要求原信噪比不能太差。

#### 信道临界带宽$W_0$

$$
N=N_0W
$$

> $N_0$表示单边功率密度； 

当$N=P=N_0W_0$的时候，$W=W_0$。

此时信道容量C和临界带宽$W_0$之比：
$$
\frac{C}{W_0}=\frac{W}{W_0}\log \left(1+\frac{P}{N}\right)=\frac{W}{W_0}\log \left(1+\frac{W_0}{W}\right)
$$
![](https://i.loli.net/2019/06/16/5d05194fdcadc57992.png)

![](https://i.loli.net/2019/06/16/5d05199f9d36223891.png)

#### 哪个接收信号最可靠问题

$$
p(x_i|y_i)
$$

higher is better.

#### 对于一般的计算大杂烩：H(X),H(Y),I(X;Y),H(X,Y)

只要算：

- $P(x_i)$
- $P(y_i)$
- $P(x_i,y_j)$

